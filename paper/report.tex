\documentclass[10pt,twocolumn]{article} % letterpaper is american!
%\documentclass[12pt]{article} % letterpaper is american!
%
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage[british,UKenglish,USenglish,english,american]{babel}

\pagestyle{empty}

\usepackage[affil-it]{authblk}

\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}

\usepackage{amsfonts,amsmath,amsthm,amssymb}

\usepackage{tikz,pgf}
\usetikzlibrary{fit}

\setlength{\parindent}{0mm}
%\usepackage{showframe}

\usepackage{multicol}
\usepackage{enumerate}

\usepackage{verbatim}

\usepackage{xspace}
\usepackage{url}
\usepackage{cite}

\usepackage{coffee4}

\usepackage{titlesec}
\titlespacing*{\subsubsection}{0pt}{*0}{*0}
\titlespacing*{\subsection}{0pt}{0pt}{*0}
\titlespacing*{\section}{0pt}{0pt}{*0}

\newcommand{\Bold}{\mathbf}

\setlength{\parskip}{1em}
\setlength{\parindent}{1em}

\title{Cluster Rank Demo Harness}
\date{\today}
\author{Philip Robinson}
\affil{Oregon Health Sciences University}

\begin{document}
\maketitle
\cofeAm{0.2}{0.9}{0}{5.5cm}{3cm}
\cofeCm{0.1}{1}{180}{0}{0}
\begin{abstract}
  It is often the case that initial query compositions result in frequent restarts as
  the user negotiates with their retrieval system. This is likely a product of unfortunate
  query formulations or choice of ranking algorithm. Our proposed retrieval system
  encourages diversity in displayed documents by introducing an unsupervised clustering
  step before displaying results. The clusters are then presented to the user with their
  documents ranked independent of each group. We do this by clearly seperating the
  retrieval process into the three steps \texttt{relevance}, \texttt{clustering}, and
  \texttt{ranking}, then allow the user to recurse this process on a cluster (rather
  than restarting their query). Additionally, we propose a simple method to
  compare results against varying quality tfidf queries. Our final product is a demo
  harness that abstracts these steps, so that others may easily produce and reproduce
  prototypes against their own corpora.
\end{abstract}

%\section*{General Terms}
%\section*{Keywords}
\section{Introduction}
Information retrieval systems have long suffered from non-informative query formulations
by their users. Many systems employ techniques such as query expansion, domain ontologies,
advanced search parameters, to address such difficulties. Unfortunately, most retrieval
systems presume user provided queries to be informative, and select to return the most
query-relevent documents. We can interpret a query-relevance ranking on documents retrieved
by a non-informative query as being an overfit ranking (to the query). Unfortunately, this
overfitting can happen regardless of query quality. In cases where all documents retrieved
are nearly identical.

Either in the case of non-informated queries or of monolithic document responses, users
are often tasked with query reformulation. Reformulation may be an inssuficent mechanism,
for users not appropriately familure with their target domain. Additionally, as
a side effect of this workflow, query reformulation can make it difficult to assess
the general effectiveness of a retrieval system\footnote{reference}.

To prevent overfit rankings some retrieval systems have proposed document diversification
strategies\footnote{reference}. This can easily be done by similarity or ontological
clustering of documents either prior to or after query submission. Additionally, to improve
query formulation many search engines provide similar search terms along side retrieved
documents\footnote{reference}.

%We argue that relevence is unlikely to change

%We are interested in combining these two strategies by providing unsupervised clustering
%on relevent


%A common means of reducing overfit ranking, and lessoning the query burden of the user is
%to introduce document diversification techniques.

%Document diversification can be used to reduce instances of overfitting to non-informative queries. 


%The modern search engines are faced with the
%enormous task of returning the few most relevant search
%results based on user query. In general the search results
%returned using any searching paradigm are not clustered
%automatically. 

\section{Implementation Details}
\section{Evaluation Approach}
\section{Expirimental Results}
\section{Limitations}
\section{References}

\end{document}
